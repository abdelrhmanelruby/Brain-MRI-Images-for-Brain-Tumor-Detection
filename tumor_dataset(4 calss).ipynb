{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img ,img_to_array\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Conv2D,Dropout,MaxPooling2D,Flatten,Dense,BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import keras\n",
    "import shutil\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "datadir = r'Brain Tumor Classification (MRI)/Training'\n",
    "classes = os.listdir(datadir)\n",
    "Categories = []\n",
    "for file_label in os.listdir(datadir):\n",
    "    Categories.append(file_label)\n",
    "    for file_name in os.listdir(datadir+'/'+file_label):\n",
    "        x.append(datadir+'/'+file_label+'/'+file_name)\n",
    "        y.append(file_label)\n",
    "\n",
    "train_df=pd.DataFrame({\n",
    "        'image_path' : x,\n",
    "        'label' : y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "datadir = r'Brain Tumor Classification (MRI)/Testing'\n",
    "Categories = []\n",
    "for file_label in os.listdir(datadir):\n",
    "    Categories.append(file_label)\n",
    "    for file_name in os.listdir(datadir+'/'+file_label):\n",
    "        x.append(datadir+'/'+file_label+'/'+file_name)\n",
    "        y.append(file_label)\n",
    "\n",
    "test_df=pd.DataFrame({\n",
    "        'image_path' : x,\n",
    "        'label' : y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.12, shuffle=True, random_state=123, stratify=train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df =  2525\n",
      "valid_df =  345\n",
      "test_df =  394\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df = \",len(train_df))\n",
    "print(\"valid_df = \",len(valid_df))\n",
    "print(\"test_df = \",len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a=0\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in classes:\n",
    "    ax = plt.subplot(6,3, a+1)\n",
    "    plt.tight_layout()\n",
    "    path=os.path.join(datadir+'/'+i)\n",
    "    img = plt.imread(path +'/'+ random.choice(sorted(os.listdir(path))))\n",
    "    plt.imshow(img)\n",
    "    plt.title(i,fontsize=6,weight='bold')\n",
    "    plt.axis('off')\n",
    "    a+=1\n",
    "plt.savefig('models/4class/sample_17.png',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(horizontal_flip= True)\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2525 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_dataframe( train_df, \n",
    "                                       x_col= 'image_path', \n",
    "                                       y_col= 'label', \n",
    "                                       target_size= (224,224), \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= \"rgb\", \n",
    "                                       shuffle= True, \n",
    "                                       batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 345 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "vald_set = test_datagen.flow_from_dataframe( valid_df, \n",
    "                                       x_col= 'image_path', \n",
    "                                       y_col= 'label', \n",
    "                                       target_size= (224,224), \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= \"rgb\", \n",
    "                                       shuffle= True, \n",
    "                                       batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_dataframe( test_df, \n",
    "                                      x_col= 'image_path', \n",
    "                                      y_col= 'label', \n",
    "                                      target_size= (224,224), \n",
    "                                      class_mode= 'categorical',\n",
    "                                      color_mode= \"rgb\", \n",
    "                                      shuffle= False, \n",
    "                                      batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.models.load_model('44class_96.5.h5')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.pop()\n",
    "base_model.pop()\n",
    "base_model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb5 (Functional)  (None, 2048)             28513527  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,521,719\n",
      "Trainable params: 0\n",
      "Non-trainable params: 28,521,719\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "    Dense(256, \n",
    "          kernel_regularizer= regularizers.l2(l= 0.016), \n",
    "          activity_regularizer= regularizers.l1(0.006),\n",
    "          bias_regularizer= regularizers.l1(0.006), \n",
    "          activation= 'relu'),\n",
    "    \n",
    "    Dropout(rate= 0.45, \n",
    "            seed= 123),\n",
    "    Dense(4, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' model = Sequential()\\nmodel.add(base_model)\\nmodel.add(Dense(2, activation=\"softmax\")) '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(2, activation=\"softmax\")) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 2048)              28521719  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,055,483\n",
      "Trainable params: 529,668\n",
      "Non-trainable params: 28,525,815\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "checkpoint_name = 'Best-val_accuracy.h5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_accuracy', verbose = 1, save_best_only = True,save_weights_only=True, mode ='auto')\n",
    "\n",
    "checkpoint_name1 = 'Best-val_loss.h5' \n",
    "checkpoint1 = ModelCheckpoint(checkpoint_name1, monitor='val_loss', verbose = 1, save_best_only = True,save_weights_only=True, mode ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 7.9084 - accuracy: 0.7663\n",
      "Epoch 1: val_accuracy improved from -inf to 0.85507, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 6.69331, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 40s 334ms/step - loss: 7.9084 - accuracy: 0.7663 - val_loss: 6.6933 - val_accuracy: 0.8551\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 6.1116 - accuracy: 0.8653\n",
      "Epoch 2: val_accuracy improved from 0.85507 to 0.89275, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 6.69331 to 5.38138, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 22s 279ms/step - loss: 6.1116 - accuracy: 0.8653 - val_loss: 5.3814 - val_accuracy: 0.8928\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.9301 - accuracy: 0.8950\n",
      "Epoch 3: val_accuracy improved from 0.89275 to 0.92464, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 5.38138 to 4.38806, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 22s 281ms/step - loss: 4.9301 - accuracy: 0.8950 - val_loss: 4.3881 - val_accuracy: 0.9246\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.0319 - accuracy: 0.9069\n",
      "Epoch 4: val_accuracy did not improve from 0.92464\n",
      "\n",
      "Epoch 4: val_loss improved from 4.38806 to 3.61306, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 246ms/step - loss: 4.0319 - accuracy: 0.9069 - val_loss: 3.6131 - val_accuracy: 0.9159\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.2813 - accuracy: 0.9244\n",
      "Epoch 5: val_accuracy did not improve from 0.92464\n",
      "\n",
      "Epoch 5: val_loss improved from 3.61306 to 2.97475, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 248ms/step - loss: 3.2813 - accuracy: 0.9244 - val_loss: 2.9747 - val_accuracy: 0.9217\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7089 - accuracy: 0.9315\n",
      "Epoch 6: val_accuracy did not improve from 0.92464\n",
      "\n",
      "Epoch 6: val_loss improved from 2.97475 to 2.46167, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 251ms/step - loss: 2.7089 - accuracy: 0.9315 - val_loss: 2.4617 - val_accuracy: 0.9188\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.2251 - accuracy: 0.9426\n",
      "Epoch 7: val_accuracy improved from 0.92464 to 0.92754, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 2.46167 to 2.04388, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 21s 271ms/step - loss: 2.2251 - accuracy: 0.9426 - val_loss: 2.0439 - val_accuracy: 0.9275\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.8555 - accuracy: 0.9489\n",
      "Epoch 8: val_accuracy improved from 0.92754 to 0.93043, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 2.04388 to 1.72187, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 22s 278ms/step - loss: 1.8555 - accuracy: 0.9489 - val_loss: 1.7219 - val_accuracy: 0.9304\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.5616 - accuracy: 0.9446\n",
      "Epoch 9: val_accuracy improved from 0.93043 to 0.94783, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.72187 to 1.45139, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 21s 266ms/step - loss: 1.5616 - accuracy: 0.9446 - val_loss: 1.4514 - val_accuracy: 0.9478\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.3298 - accuracy: 0.9521\n",
      "Epoch 10: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 10: val_loss improved from 1.45139 to 1.25082, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 1.3298 - accuracy: 0.9521 - val_loss: 1.2508 - val_accuracy: 0.9391\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.1264 - accuracy: 0.9612\n",
      "Epoch 11: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 11: val_loss improved from 1.25082 to 1.08398, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 246ms/step - loss: 1.1264 - accuracy: 0.9612 - val_loss: 1.0840 - val_accuracy: 0.9333\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.9897 - accuracy: 0.9549\n",
      "Epoch 12: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08398 to 0.95952, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.9897 - accuracy: 0.9549 - val_loss: 0.9595 - val_accuracy: 0.9449\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8758 - accuracy: 0.9560\n",
      "Epoch 13: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 13: val_loss improved from 0.95952 to 0.86821, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.8758 - accuracy: 0.9560 - val_loss: 0.8682 - val_accuracy: 0.9362\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7727 - accuracy: 0.9596\n",
      "Epoch 14: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 14: val_loss improved from 0.86821 to 0.78948, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.7727 - accuracy: 0.9596 - val_loss: 0.7895 - val_accuracy: 0.9391\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.9636\n",
      "Epoch 15: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 15: val_loss improved from 0.78948 to 0.73450, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.6979 - accuracy: 0.9636 - val_loss: 0.7345 - val_accuracy: 0.9275\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6448 - accuracy: 0.9636\n",
      "Epoch 16: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 16: val_loss improved from 0.73450 to 0.66939, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 250ms/step - loss: 0.6448 - accuracy: 0.9636 - val_loss: 0.6694 - val_accuracy: 0.9362\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6008 - accuracy: 0.9600\n",
      "Epoch 17: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 17: val_loss improved from 0.66939 to 0.61638, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 251ms/step - loss: 0.6008 - accuracy: 0.9600 - val_loss: 0.6164 - val_accuracy: 0.9449\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.9687\n",
      "Epoch 18: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 18: val_loss improved from 0.61638 to 0.58483, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 246ms/step - loss: 0.5483 - accuracy: 0.9687 - val_loss: 0.5848 - val_accuracy: 0.9391\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.9648\n",
      "Epoch 19: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 19: val_loss improved from 0.58483 to 0.55428, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.5298 - accuracy: 0.9648 - val_loss: 0.5543 - val_accuracy: 0.9449\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.9644\n",
      "Epoch 20: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 20: val_loss improved from 0.55428 to 0.54674, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.5012 - accuracy: 0.9644 - val_loss: 0.5467 - val_accuracy: 0.9420\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.9707\n",
      "Epoch 21: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 21: val_loss improved from 0.54674 to 0.52620, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 246ms/step - loss: 0.4718 - accuracy: 0.9707 - val_loss: 0.5262 - val_accuracy: 0.9333\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.9667\n",
      "Epoch 22: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 22: val_loss improved from 0.52620 to 0.51404, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.4652 - accuracy: 0.9667 - val_loss: 0.5140 - val_accuracy: 0.9304\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.9683\n",
      "Epoch 23: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 23: val_loss improved from 0.51404 to 0.48259, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 252ms/step - loss: 0.4405 - accuracy: 0.9683 - val_loss: 0.4826 - val_accuracy: 0.9478\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9675\n",
      "Epoch 24: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 24: val_loss improved from 0.48259 to 0.48228, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 252ms/step - loss: 0.4320 - accuracy: 0.9675 - val_loss: 0.4823 - val_accuracy: 0.9420\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.9711\n",
      "Epoch 25: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 25: val_loss improved from 0.48228 to 0.46880, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.4065 - accuracy: 0.9711 - val_loss: 0.4688 - val_accuracy: 0.9304\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.9695\n",
      "Epoch 26: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 26: val_loss improved from 0.46880 to 0.45821, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 248ms/step - loss: 0.4041 - accuracy: 0.9695 - val_loss: 0.4582 - val_accuracy: 0.9333\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.9671\n",
      "Epoch 27: val_accuracy did not improve from 0.94783\n",
      "\n",
      "Epoch 27: val_loss improved from 0.45821 to 0.44438, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.4037 - accuracy: 0.9671 - val_loss: 0.4444 - val_accuracy: 0.9449\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9766\n",
      "Epoch 28: val_accuracy improved from 0.94783 to 0.95362, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.44438 to 0.44328, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 21s 268ms/step - loss: 0.3765 - accuracy: 0.9766 - val_loss: 0.4433 - val_accuracy: 0.9536\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.9695\n",
      "Epoch 29: val_accuracy did not improve from 0.95362\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.44328\n",
      "79/79 [==============================] - 19s 239ms/step - loss: 0.3831 - accuracy: 0.9695 - val_loss: 0.4512 - val_accuracy: 0.9449\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.9648\n",
      "Epoch 30: val_accuracy did not improve from 0.95362\n",
      "\n",
      "Epoch 30: val_loss improved from 0.44328 to 0.44097, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.3882 - accuracy: 0.9648 - val_loss: 0.4410 - val_accuracy: 0.9478\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.9663\n",
      "Epoch 31: val_accuracy improved from 0.95362 to 0.95942, saving model to Best-val_accuracy.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.44097 to 0.41947, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 21s 271ms/step - loss: 0.3837 - accuracy: 0.9663 - val_loss: 0.4195 - val_accuracy: 0.9594\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.9723\n",
      "Epoch 32: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 32: val_loss improved from 0.41947 to 0.41852, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.3595 - accuracy: 0.9723 - val_loss: 0.4185 - val_accuracy: 0.9478\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.9766\n",
      "Epoch 33: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.41852\n",
      "79/79 [==============================] - 19s 237ms/step - loss: 0.3607 - accuracy: 0.9766 - val_loss: 0.4240 - val_accuracy: 0.9478\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9739\n",
      "Epoch 34: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.41852\n",
      "79/79 [==============================] - 19s 237ms/step - loss: 0.3468 - accuracy: 0.9739 - val_loss: 0.4362 - val_accuracy: 0.9449\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.9679\n",
      "Epoch 35: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.41852\n",
      "79/79 [==============================] - 19s 238ms/step - loss: 0.3524 - accuracy: 0.9679 - val_loss: 0.4242 - val_accuracy: 0.9275\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.9715\n",
      "Epoch 36: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.41852\n",
      "79/79 [==============================] - 19s 238ms/step - loss: 0.3433 - accuracy: 0.9715 - val_loss: 0.4191 - val_accuracy: 0.9391\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.9675\n",
      "Epoch 37: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 37: val_loss improved from 0.41852 to 0.40806, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 250ms/step - loss: 0.3496 - accuracy: 0.9675 - val_loss: 0.4081 - val_accuracy: 0.9333\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.9632\n",
      "Epoch 38: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.40806\n",
      "79/79 [==============================] - 19s 241ms/step - loss: 0.3534 - accuracy: 0.9632 - val_loss: 0.4103 - val_accuracy: 0.9420\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.9715\n",
      "Epoch 39: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 39: val_loss improved from 0.40806 to 0.40217, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 255ms/step - loss: 0.3352 - accuracy: 0.9715 - val_loss: 0.4022 - val_accuracy: 0.9420\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9770\n",
      "Epoch 40: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 40: val_loss improved from 0.40217 to 0.39765, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 20s 247ms/step - loss: 0.3266 - accuracy: 0.9770 - val_loss: 0.3976 - val_accuracy: 0.9362\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.9782\n",
      "Epoch 41: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.39765\n",
      "79/79 [==============================] - 19s 237ms/step - loss: 0.3240 - accuracy: 0.9782 - val_loss: 0.4076 - val_accuracy: 0.9478\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9830\n",
      "Epoch 42: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.39765\n",
      "79/79 [==============================] - 19s 239ms/step - loss: 0.3073 - accuracy: 0.9830 - val_loss: 0.3981 - val_accuracy: 0.9420\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9794\n",
      "Epoch 43: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 43: val_loss improved from 0.39765 to 0.39191, saving model to Best-val_loss.h5\n",
      "79/79 [==============================] - 19s 246ms/step - loss: 0.3068 - accuracy: 0.9794 - val_loss: 0.3919 - val_accuracy: 0.9507\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.9707\n",
      "Epoch 44: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.39191\n",
      "79/79 [==============================] - 19s 239ms/step - loss: 0.3196 - accuracy: 0.9707 - val_loss: 0.4156 - val_accuracy: 0.9391\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.9758\n",
      "Epoch 45: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.39191\n",
      "79/79 [==============================] - 19s 238ms/step - loss: 0.3174 - accuracy: 0.9758 - val_loss: 0.3949 - val_accuracy: 0.9420\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.9758\n",
      "Epoch 46: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.39191\n",
      "79/79 [==============================] - 18s 233ms/step - loss: 0.3260 - accuracy: 0.9758 - val_loss: 0.4157 - val_accuracy: 0.9449\n",
      "Epoch 47/300\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.9731\n",
      "Epoch 47: val_accuracy did not improve from 0.95942\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.39191\n",
      "79/79 [==============================] - 18s 232ms/step - loss: 0.3243 - accuracy: 0.9731 - val_loss: 0.4068 - val_accuracy: 0.9304\n",
      "Epoch 48/300\n",
      "47/79 [================>.............] - ETA: 6s - loss: 0.3053 - accuracy: 0.9774"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(training_set,validation_data \u001b[39m=\u001b[39;49m vald_set, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,callbacks\u001b[39m=\u001b[39;49m[checkpoint,checkpoint1,early_stopping])\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Ruby\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = model.fit(training_set,validation_data = vald_set, epochs=300,shuffle=False,callbacks=[checkpoint,checkpoint1,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Best-val_loss.h5')\n",
    "#model = keras.models.load_model('Weights-038-val_loss-0.43673.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 16s 203ms/step - loss: 0.2911 - accuracy: 0.9976\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.4195 - accuracy: 0.9594\n",
      "13/13 [==============================] - 4s 286ms/step - loss: 1.1992 - accuracy: 0.7690\n",
      "Train Loss:  0.29113808274269104\n",
      "Train Accuracy:  0.9976237416267395\n",
      "--------------------\n",
      "Validation Loss:  0.4194658696651459\n",
      "Validation Accuracy:  0.9594202637672424\n",
      "--------------------\n",
      "Test Loss:  1.1991552114486694\n",
      "Test Accuracy:  0.7690355181694031\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(training_set)\n",
    "valid_score = model.evaluate(vald_set)\n",
    "test_score = model.evaluate(test_set)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 16s 199ms/step - loss: 0.2427 - accuracy: 0.9980\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.3919 - accuracy: 0.9507\n",
      "13/13 [==============================] - 3s 190ms/step - loss: 1.1762 - accuracy: 0.7690\n",
      "Train Loss:  0.24268269538879395\n",
      "Train Accuracy:  0.998019814491272\n",
      "--------------------\n",
      "Validation Loss:  0.3919147551059723\n",
      "Validation Accuracy:  0.9507246613502502\n",
      "--------------------\n",
      "Test Loss:  1.1762440204620361\n",
      "Test Accuracy:  0.7690355181694031\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(training_set)\n",
    "valid_score = model.evaluate(vald_set)\n",
    "test_score = model.evaluate(test_set)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('2class_98.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('2calss_lagre_dataset_99.1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    \"0a0bc6879f5d5d14c4df229b64b801_big_gallery.jpeg\",\n",
    "    grayscale=False,\n",
    "    color_mode='rgb',\n",
    "    target_size=(224,224),\n",
    "    interpolation='nearest',\n",
    "    keep_aspect_ratio=False\n",
    "    )\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(img).reshape(-1, 224, 224, 3)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(img)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=['Astrocitoma T1',\n",
    " 'Astrocitoma T1C+',\n",
    " 'Astrocitoma T2',\n",
    " 'Carcinoma T1',\n",
    " 'Carcinoma T1C+',\n",
    " 'Carcinoma T2',\n",
    " 'Ependimoma T1',\n",
    " 'Ependimoma T1C+',\n",
    " 'Ependimoma T2',\n",
    " 'Ganglioglioma T1',\n",
    " 'Ganglioglioma T1C+',\n",
    " 'Ganglioglioma T2',\n",
    " 'Germinoma T1',\n",
    " 'Germinoma T1C+',\n",
    " 'Germinoma T2',\n",
    " 'Glioblastoma T1',\n",
    " 'Glioblastoma T1C+',\n",
    " 'Glioblastoma T2',\n",
    " 'Granuloma T1',\n",
    " 'Granuloma T1C+',\n",
    " 'Granuloma T2',\n",
    " 'Meduloblastoma T1',\n",
    " 'Meduloblastoma T1C+',\n",
    " 'Meduloblastoma T2',\n",
    " 'Meningioma T1',\n",
    " 'Meningioma T1C+',\n",
    " 'Meningioma T2',\n",
    " 'Neurocitoma T1',\n",
    " 'Neurocitoma T1C+',\n",
    " 'Neurocitoma T2',\n",
    " 'Oligodendroglioma T1',\n",
    " 'Oligodendroglioma T1C+',\n",
    " 'Oligodendroglioma T2',\n",
    " 'Papiloma T1',\n",
    " 'Papiloma T1C+',\n",
    " 'Papiloma T2',\n",
    " 'Schwannoma T1',\n",
    " 'Schwannoma T1C+',\n",
    " 'Schwannoma T2',\n",
    " 'Tuberculoma T1',\n",
    " 'Tuberculoma T1C+',\n",
    " 'Tuberculoma T2',\n",
    " '_NORMAL T1',\n",
    " '_NORMAL T2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[y_pred_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(res.history['loss'], 'r', label= 'Training loss')\n",
    "plt.plot(res.history['val_loss'], 'b', label= 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(res.history['accuracy'], 'r', label= 'Training Accuracy')\n",
    "plt.plot(res.history['val_accuracy'], 'b', label= 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout\n",
    "plt.savefig('EfficientNetB5_4calss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
